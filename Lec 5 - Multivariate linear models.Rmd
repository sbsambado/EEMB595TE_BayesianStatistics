---
title: "Lec 5 - Multivariate linear models"
author: "sbsambado"
date: "4/15/2020"
output: html_document
---

Lec 5 by Richard McElreath based on his book Statistical Rethinking\
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Correlation is commonplace.
- Waffle house density is correlated with divorce rate (i.e. correlation but not causation)

Multiple regression models
- the good
  - reveal spurious correlation
  - uncover masked association
- the bad
  - cause spurious correlation
  - hide real association
- learn basics of causal inference
  - directed acyclic graphs
  - forks, pipes, colliders
  - backdoor criterion
  
  
Spurious association

Multiple regression 
- want to know: what is value of predictor, once we know the other predictors?
  i.e. what is value o f knowing marriage rate, once we already know the median age of marriage?
  
Directed acycling graphs (DAGs)
- tools for causal models
  - directed : arrows
  - acyclic : arrows don't make loops
  - graphs: nodes and edges
- unlike statistical model (no direction), has causal implications


Attempt to make a schematic (lol)
(median age of marriage) A ----> M (marriage rate)
                         |       |
                         \       /
                          > D   <  (divorce rate)
        
  Implications
    (1) M is a function of A
    (2) D is a function of A and M
    (3) The total causal effect of A has two paths
      (a) A -> M -> D (indirect)
      (b) A -> D (direct)
      
path: from a variable to another variable


Good DAGs
- given association M <-> D, cannot tell difference between path a or b
  - need conditional association: M <-> D| A
  
Priors
- standardize variables (divorce rate, D; marriage rate Ml median age at marriage A)
- expect alpha to be near zero [ alpha ~ Normal(0, 0.2)]
- slopws should not produce impossibly strong relationships
  Bm ~ Normal(0, 0.5)
  Ba ~ Normal(0, 0.5)
 
Prior predictive simulation 
```{r}

# flattest prior you could justify, not plausible though

# R code 5.3
library(rethinking)
m5.1 <- quap( # age of marriage only D ~ A
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA * A ,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ) , data = d)

# R code 5.4

set.seed(10)
prior <- extract.prior(m5.1) # use rnorm to sample
mu <- link(m5.1, post = prior, data = list(A = c(-2, 2)))
plot(NULL, xlim = c(-2, 2))
for(i in 1:50) lines(c(-2,2), mu[i,] , col = col.alpha('black', 0.4))
```
~ remember: linear means addative, no coefficient

How to visualize multivariate models?

Posterior predictions
- lots of plotting options
  1. predictor residual plots
  2. counterfactual plots
  3. posterior prediction plots


1. Predictor residual plots
- goal: show association of each predictor with outcome, 'controlling' for toher predictors
- useful intuition
- never analyze residuals!
- recipe
  1. regress predictor on other predictors
  2. compute predictor residuals
  3. regress outcome on residuals


distance of line = residual

residual is a distribution, not a single value

Statistical control
- multiple linear regression answers questions: how is each predictor associated wiht outcome, once we know all the other predictors?
  - uses model to build expected outcomes - not magic
  - can't make strong causal inferences from averages; need data on individuals
  
Counterfactual plots
- goal: explore model implications for outcomes
  - fix other predictor(s)
  - compute predictions across values of predictor
- compute for unobserved (impossible?) cases, hence 'counterfactual'

posterior prediction checks
- goal: compute implied predictions for observed cases
  - check model fit 
  - find model failures
- always average over the posterior distribution
  - using only posterior mean leads to overconfidence
  - embrace the uncertainty

masked association
- sometimes association between outcome and predictor by another variable
- need both variables to see influence of either
- tends to arise when
  - another predictor associated with outcome in opposite direction
  - both predictors associated with one another
- noise in predictors can also mask association (residual confounding)

masked influence of milk on brain cortex
```{r}
data(milk)
d <- milk
pairs(~kcal.per.g + log(mass) +
        neocortex.perc, data = d)
```

synthetic masked association
```{r}
# M -> K <- N
# M <- U -> N

n <- 100
U <- rnorm(n)
N <- rnorm(n, U)
M <- rnorm(n, U)
K <- rnorm(n, N - M)
d_sim3 <- data.frame(K = K, N = N, M = M)
```

Cateforical variables
- many predictors are discrete, unordered categories
  - gender, region, species
- how to use in regression?
  - two approaches
    - use dummy/indicator variables
    - use index variables
  - most automated software uses dummy variables
  - usually easier to think and code with index variables

Dummy (indicator) variables
- variables that use 1 to indicate a category and 0 to indicate some other category
- allow each category to have a unique intercept
- coefficient is the dfference from baseline category
- problems
  - for k categories, need k - 1 dummy variables
  - makes one of the categories a priori more uncertain than others

Index variables
```{r}
# R code 5.36
d$sex <- ifelse(d$male==1, 2, 1)
str(d$sex)
```

